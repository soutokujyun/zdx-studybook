ğŸš€ å¿«é€Ÿå¼€å§‹
å‰ç½®è¦æ±‚
Docker Desktop20.10+
è‡³å°‘ 8GB å¯ç”¨å†…å­˜
è‡³å°‘ 10GB å¯ç”¨ç£ç›˜ç©ºé—´
macOS 10.15+ æˆ– Windows 10/11
1. å…‹éš†/ä¸‹è½½é¡¹ç›®
# åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir ai-local-env
cd ai-local-env

# ä¸‹è½½é…ç½®æ–‡ä»¶
curl -O https://raw.githubusercontent.com/your-repo/docker-compose.yml
curl -O https://raw.githubusercontent.com/your-repo/start.sh
curl -O https://raw.githubusercontent.com/your-repo/start.ps1
curl -O https://raw.githubusercontent.com/your-repo/start.bat
2. å¯åŠ¨æœåŠ¡
macOS / Linux:
# ç»™è„šæœ¬æ‰§è¡Œæƒé™
chmod +x start.sh

# å¯åŠ¨æœåŠ¡
./start.sh
Windows:
åŒå‡»è¿è¡Œ start.bat
æˆ–ä»¥ç®¡ç†å‘˜èº«ä»½è¿è¡Œ PowerShell è„šæœ¬ï¼š
.\start.ps1
3. ä¸‹è½½æ¨¡å‹
å¯åŠ¨è„šæœ¬ä¼šæç¤ºæ˜¯å¦ä¸‹è½½ Qwen3:1.7B æ¨¡å‹ã€‚å¦‚æœè·³è¿‡ï¼Œå¯æ‰‹åŠ¨ä¸‹è½½ï¼š
# è¿›å…¥å®¹å™¨ä¸‹è½½
docker exec ollama ollama pull qwen3:1.7b

# æˆ–é€šè¿‡ API ä¸‹è½½
curl http://localhost:11434/api/pull -d '{
  "name": "qwen3:1.7b"
}'
ğŸ”§ æœåŠ¡ä¿¡æ¯
æœåŠ¡
	
ç«¯å£
	
API åœ°å€
	
è¯´æ˜


Ollama
	
11434
	
http://localhost:11434
	
å¤§è¯­è¨€æ¨¡å‹æœåŠ¡


ChromaDB
	
8000
	
http://localhost:8000
	
å‘é‡æ•°æ®åº“æœåŠ¡
ğŸ“ é¡¹ç›®ç»“æ„
ai-local-env/
â”œâ”€â”€ docker-compose.yml    # Docker Compose é…ç½®
â”œâ”€â”€ start.sh             # macOS/Linux å¯åŠ¨è„šæœ¬
â”œâ”€â”€ start.ps1            # Windows PowerShell è„šæœ¬
â”œâ”€â”€ start.bat            # Windows æ‰¹å¤„ç†è„šæœ¬
â”œâ”€â”€ data/                # æŒä¹…åŒ–æ•°æ®ç›®å½•ï¼ˆè‡ªåŠ¨åˆ›å»ºï¼‰
â”‚   â”œâ”€â”€ ollama/         # Ollama æ¨¡å‹æ•°æ®
â”‚   â””â”€â”€ chromadb/       # ChromaDB å‘é‡æ•°æ®
â””â”€â”€ README.md           # æœ¬æ–‡æ¡£
âš¡ å¸¸ç”¨å‘½ä»¤
æœåŠ¡ç®¡ç†
# å¯åŠ¨æ‰€æœ‰æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æœåŠ¡çŠ¶æ€
docker-compose ps

# æŸ¥çœ‹æœåŠ¡æ—¥å¿—
docker-compose logs -f ollama      # Ollama æ—¥å¿—
docker-compose logs -f chromadb    # ChromaDB æ—¥å¿—
docker-compose logs -f            # æ‰€æœ‰æœåŠ¡æ—¥å¿—

# åœæ­¢æœåŠ¡
docker-compose stop

# åœæ­¢å¹¶åˆ é™¤å®¹å™¨
docker-compose down

# åœæ­¢å¹¶åˆ é™¤å®¹å™¨å’Œæ‰€æœ‰æ•°æ®
docker-compose down -v

# é‡å¯æœåŠ¡
docker-compose restart
Ollama æ“ä½œ
# æŸ¥çœ‹å·²å®‰è£…æ¨¡å‹
curl http://localhost:11434/api/tags

# ä¸‹è½½æ¨¡å‹
docker exec ollama ollama pull qwen3:1.7b
docker exec ollama ollama pull llama3:8b
docker exec ollama ollama pull mistral:7b

# è¿è¡Œæ¨¡å‹ï¼ˆäº¤äº’å¼å¯¹è¯ï¼‰
docker exec -it ollama ollama run qwen3:1.7b

# åˆ é™¤æ¨¡å‹
docker exec ollama ollama rm qwen3:1.7b

# æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯
curl http://localhost:11434/api/show -d '{
  "name": "qwen3:1.7b"
}'
ChromaDB æ“ä½œ
# æ£€æŸ¥æœåŠ¡çŠ¶æ€
curl http://localhost:8000/api/v1/heartbeat

# åˆ›å»ºé›†åˆ
curl -X POST http://localhost:8000/api/v1/collections \
  -H "Content-Type: application/json" \
  -d '{"name": "my_collection"}'

# åˆ—å‡ºæ‰€æœ‰é›†åˆ
curl http://localhost:8000/api/v1/collections

# æ·»åŠ æ–‡æ¡£
curl -X POST http://localhost:8000/api/v1/collections/my_collection/add \
  -H "Content-Type: application/json" \
  -d '{
    "documents": ["æ–‡æ¡£å†…å®¹1", "æ–‡æ¡£å†…å®¹2"],
    "metadatas": [{"source": "doc1"}, {"source": "doc2"}],
    "ids": ["id1", "id2"]
  }'

# æŸ¥è¯¢ç›¸ä¼¼æ–‡æ¡£
curl -X POST http://localhost:8000/api/v1/collections/my_collection/query \
  -H "Content-Type: application/json" \
  -d '{
    "query_texts": ["æŸ¥è¯¢å†…å®¹"],
    "n_results": 5
  }'
é›†æˆä½¿ç”¨ç¤ºä¾‹
# test_integration.py
import requests
import json

# 1. é€šè¿‡ Ollama ç”Ÿæˆå‘é‡
def generate_embedding(text, model="qwen3:1.7b"):
    response = requests.post("http://localhost:11434/api/embeddings", 
                           json={"model": model, "prompt": text})
    return response.json()["embedding"]

# 2. å­˜å‚¨åˆ° ChromaDB
def store_in_chromadb(text, embedding, collection="documents"):
    data = {
        "documents": [text],
        "embeddings": [embedding],
        "metadatas": [{"source": "ollama"}],
        "ids": [f"doc_{hash(text)}"]
    }
    response = requests.post(f"http://localhost:8000/api/v1/collections/{collection}/add", 
                           json=data)
    return response.json()

# 3. æ£€ç´¢å’Œç”Ÿæˆ
def rag_query(question):
    # ç”Ÿæˆé—®é¢˜çš„å‘é‡
    query_embedding = generate_embedding(question)
    
    # åœ¨ ChromaDB ä¸­æ£€ç´¢ç›¸ä¼¼æ–‡æ¡£
    response = requests.post("http://localhost:8000/api/v1/collections/documents/query",
                           json={
                               "query_embeddings": [query_embedding],
                               "n_results": 3
                           })
    
    # æ„å»ºä¸Šä¸‹æ–‡
    context = "\n".join([doc["documents"][0] for doc in response.json()])
    
    # è®©æ¨¡å‹åŸºäºä¸Šä¸‹æ–‡å›ç­”é—®é¢˜
    prompt = f"""åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ï¼š

{context}

é—®é¢˜ï¼š{question}
ç­”æ¡ˆï¼š"""
    
    response = requests.post("http://localhost:11434/api/generate",
                          json={
                              "model": "qwen3:1.7b",
                              "prompt": prompt,
                              "stream": False
                          })
    return response.json()["response"]

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    # å­˜å‚¨ä¸€äº›çŸ¥è¯†
    texts = [
        "Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œç”±Guido van Rossumåˆ›å»ºã€‚",
        "Dockeræ˜¯ä¸€ä¸ªå®¹å™¨åŒ–å¹³å°ï¼Œç”¨äºæ‰“åŒ…ã€åˆ†å‘å’Œè¿è¡Œåº”ç”¨ç¨‹åºã€‚",
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªåˆ†æ”¯ï¼Œä¸“æ³¨äºç®—æ³•å’Œç»Ÿè®¡æ¨¡å‹ã€‚"
    ]
    
    for text in texts:
        embedding = generate_embedding(text)
        store_in_chromadb(text, embedding)
        print(f"å·²å­˜å‚¨: {text[:50]}...")
    
    # è¿›è¡ŒRAGæŸ¥è¯¢
    result = rag_query("ä»€ä¹ˆæ˜¯Pythonï¼Ÿ")
    print(f"\nå›ç­”: {result}")
ğŸ”„ API æ¥å£
Ollama API
# ç”Ÿæˆæ–‡æœ¬
curl http://localhost:11434/api/generate -d '{
  "model": "qwen3:1.7b",
  "prompt": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±",
  "stream": false
}'

# èŠå¤©å¯¹è¯
curl http://localhost:11434/api/chat -d '{
  "model": "qwen3:1.7b",
  "messages": [
    {"role": "user", "content": "ä½ å¥½"}
  ],
  "stream": false
}'

# ç”Ÿæˆå‘é‡
curl http://localhost:11434/api/embeddings -d '{
  "model": "qwen3:1.7b",
  "prompt": "è¿™æ˜¯ä¸€æ®µæ–‡æœ¬"
}'

# å¤åˆ¶æ¨¡å‹
curl http://localhost:11434/api/copy -d '{
  "source": "qwen3:1.7b",
  "destination": "qwen3-copy:1.7b"
}'
ChromaDB API
# åˆ›å»ºé›†åˆ
curl -X POST http://localhost:8000/api/v1/collections \
  -H "Content-Type: application/json" \
  -d '{
    "name": "my_collection",
    "metadata": {"description": "æˆ‘çš„æ–‡æ¡£é›†åˆ"}
  }'

# è·å–é›†åˆ
curl http://localhost:8000/api/v1/collections/my_collection

# æ·»åŠ æ•°æ®
curl -X POST http://localhost:8000/api/v1/collections/my_collection/add \
  -H "Content-Type: application/json" \
  -d '{
    "documents": ["æ–‡æ¡£1", "æ–‡æ¡£2"],
    "metadatas": [{"type": "article"}, {"type": "paper"}],
    "ids": ["1", "2"]
  }'

# æŸ¥è¯¢
curl -X POST http://localhost:8000/api/v1/collections/my_collection/query \
  -H "Content-Type: application/json" \
  -d '{
    "query_texts": ["æŸ¥è¯¢å…³é”®è¯"],
    "n_results": 5
  }'

# æ›´æ–°
curl -X POST http://localhost:8000/api/v1/collections/my_collection/update \
  -H "Content-Type: application/json" \
  -d '{
    "documents": ["æ›´æ–°åçš„æ–‡æ¡£"],
    "metadatas": [{"type": "updated"}],
    "ids": ["1"]
  }'

# åˆ é™¤
curl -X POST http://localhost:8000/api/v1/collections/my_collection/delete \
  -H "Content-Type: application/json" \
  -d '{
    "ids": ["1"]
  }'
ğŸ³ Docker å®¹å™¨ç®¡ç†
è¿›å…¥å®¹å™¨
# è¿›å…¥ Ollama å®¹å™¨
docker exec -it ollama /bin/bash

# è¿›å…¥ ChromaDB å®¹å™¨
docker exec -it chromadb /bin/bash
æŸ¥çœ‹èµ„æºä½¿ç”¨
# æŸ¥çœ‹å®¹å™¨èµ„æºä½¿ç”¨
docker stats

# æŸ¥çœ‹å®¹å™¨è¯¦ç»†ä¿¡æ¯
docker inspect ollama

# æŸ¥çœ‹å®¹å™¨æ—¥å¿—
docker logs ollama --tail 50 -f
å¤‡ä»½å’Œæ¢å¤
# å¤‡ä»½æ•°æ®å·
docker run --rm -v ollama_data:/source -v $(pwd):/backup alpine \
  tar -czf /backup/ollama_backup.tar.gz -C /source .

# æ¢å¤æ•°æ®å·
docker run --rm -v ollama_data:/target -v $(pwd):/backup alpine \
  tar -xzf /backup/ollama_backup.tar.gz -C /target
ğŸš¨ æ•…éšœæ’é™¤
å¸¸è§é—®é¢˜
ç«¯å£å†²çª
# æ£€æŸ¥ç«¯å£å ç”¨
lsof -i :11434  # macOS/Linux
netstat -ano | findstr :11434  # Windows

# ä¿®æ”¹ docker-compose.yml ä¸­çš„ç«¯å£æ˜ å°„
ports:
  - "11435:11434"  # å°†å®¿ä¸»æœºç«¯å£æ”¹ä¸º 11435
å®¹å™¨å¯åŠ¨å¤±è´¥
# æŸ¥çœ‹è¯¦ç»†æ—¥å¿—
docker-compose logs ollama

# é‡å¯æœåŠ¡
docker-compose down
docker-compose up -d
æ¨¡å‹ä¸‹è½½å¤±è´¥
# æ£€æŸ¥ç½‘ç»œè¿æ¥
docker exec ollama ping -c 3 ollama.com

# æ‰‹åŠ¨ä¸‹è½½é•œåƒ
docker pull ollama/ollama:latest
docker pull chromadb/chroma:latest
å†…å­˜ä¸è¶³
macOS: Docker Desktop â†’ Preferences â†’ Resources â†’ Memory (â‰¥ 6GB)
Windows: Docker Desktop â†’ Settings â†’ Resources â†’ Memory (â‰¥ 6GB)
ç£ç›˜ç©ºé—´ä¸è¶³
# æ¸…ç†æœªä½¿ç”¨çš„ Docker èµ„æº
docker system prune -a

# åˆ é™¤ç‰¹å®šé•œåƒ
docker rmi <image_id>
æ—¥å¿—çº§åˆ«
# æŸ¥çœ‹è¯¦ç»†çš„ Docker æ—¥å¿—
docker-compose logs --tail=100 -f

# è¿›å…¥å®¹å™¨æŸ¥çœ‹è¿›ç¨‹
docker exec -it ollama ps aux
docker exec -it chromadb ps aux
ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–
å†…å­˜é™åˆ¶
æ ¹æ®å¯ç”¨å†…å­˜è°ƒæ•´ docker-compose.ymlï¼š
deploy:
  resources:
    limits:
      memory: 4G  # Ollama
      memory: 1G  # ChromaDB
å­˜å‚¨ä¼˜åŒ–
ä½¿ç”¨æœ¬åœ°ç›®å½•è€Œé Docker å·ï¼š
volumes:
  - ./data/ollama:/root/.ollama
  - ./data/chromadb:/chroma/chroma
ç½‘ç»œé…ç½®
ä½¿ç”¨è‡ªå®šä¹‰ç½‘ç»œæé«˜æ€§èƒ½ï¼š
networks:
  ai-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.20.0.0/16
ğŸ”„ æ›´æ–°å’Œç»´æŠ¤
æ›´æ–°é•œåƒ
# æ‹‰å–æœ€æ–°é•œåƒ
docker-compose pull

# é‡å¯æœåŠ¡
docker-compose up -d
æ¸…ç†
# æ¸…ç†æœªä½¿ç”¨çš„èµ„æº
docker system prune

# åˆ é™¤æ‰€æœ‰å®¹å™¨å’Œå·
docker-compose down -v
docker system prune -a

# åªåˆ é™¤æœªä½¿ç”¨çš„å·
docker volume prune
ç›‘æ§
# å®æ—¶ç›‘æ§
docker stats

# æŸ¥çœ‹æœåŠ¡å¥åº·
docker-compose ps
docker-compose top
ğŸ“Š æ”¯æŒçš„æ¨¡å‹
æ¨èæ¨¡å‹
æ¨¡å‹
	
å¤§å°
	
å†…å­˜éœ€æ±‚
	
è¯´æ˜


Qwen3:1.7B
	
~1.1GB
	
4-6GB
	
ä¸­æ–‡ä¼˜åŒ–ï¼Œæ¨è


Llama3.2:1B
	
~0.6GB
	
2-3GB
	
è‹±æ–‡ï¼Œé€Ÿåº¦æœ€å¿«


Gemma2:2b
	
~1.4GB
	
4-6GB
	
Google è½»é‡çº§


Phi3:mini
	
~1.8GB
	
4-6GB
	
å¾®è½¯å°æ¨¡å‹
ä¸‹è½½æ›´å¤šæ¨¡å‹
# æŸ¥çœ‹å¯ç”¨æ¨¡å‹
curl http://localhost:11434/api/tags

# ä¸‹è½½å…¶ä»–æ¨¡å‹
docker exec ollama ollama pull llama3.2:1b
docker exec ollama ollama pull gemma2:2b
docker exec ollama ollama pull phi3:mini